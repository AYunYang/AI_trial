{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "853e84d2-d914-4878-9fa8-b0a013cb2bbc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenj\\AppData\\Local\\Temp\\ipykernel_3972\\850930544.py:20: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  temperature_readings = temperature_readings.interpolate(method='linear')#fill in null values\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "#Reading Data from CSV Files\n",
    "temperature_readings = pd.read_csv('data/data.csv') #original dataset for temp readings \n",
    "temperature_readings = temperature_readings.interpolate(method='linear')#fill in null values\n",
    "\n",
    "\n",
    "weather_readings = pd.read_csv('data/weatherData.csv') #orignal dataset for weather readings\n",
    "\n",
    "# Converting date and time in temperature_readings to UNIX timestamp for comparision \n",
    "temperature_readings['datetime_str'] = temperature_readings['date'] + ' ' + temperature_readings['time']\n",
    "temperature_readings['datetime'] = temperature_readings['datetime_str'].apply(lambda x: datetime.strptime(x, \"%a %b %d %Y %I:%M:%S %p\"))\n",
    "temperature_readings['unix_timestamp'] = temperature_readings['datetime'].apply(lambda x: int(x.timestamp()))\n",
    "\n",
    "# Converting date and time in weather_readings to UNIX timestamp for comparision \n",
    "weather_readings['datetime_str'] = weather_readings['date'] + ' ' + weather_readings['time']\n",
    "weather_readings['datetime'] = weather_readings['datetime_str'].apply(lambda x: datetime.strptime(x, \"%a %b %d %Y %I:%M:%S %p\"))\n",
    "weather_readings['unix_timestamp'] = weather_readings['datetime'].apply(lambda x: int(x.timestamp()))\n",
    "\n",
    "weather_result_col = [col for col in weather_readings.columns if \"result\" in col.lower()]\n",
    "\n",
    "# Merging both data for it to be on the same time\n",
    "merged_data = pd.merge_asof(\n",
    "    temperature_readings,  # Left DataFrame\n",
    "    weather_readings[['unix_timestamp']+weather_result_col],      # Right DataFrame\n",
    "    on='unix_timestamp',   # Key column\n",
    "    direction='nearest'    # Match the nearest time\n",
    ")\n",
    "\n",
    "datetime_string = merged_data['date'] + \" \" + merged_data[\"time\"]\n",
    "merged_data[\"ISO_formatted_datetime\"] = pd.to_datetime(\n",
    "    datetime_string,\n",
    "    format=\"%a %b %d %Y %I:%M:%S %p\"\n",
    ")\n",
    "\n",
    "\n",
    "# #Columns for lorWan Sensors\n",
    "temperature_col = [\n",
    "    col for col in merged_data.columns \n",
    "    if \"lorawan_readings\" in col.lower() and \"temperature\" in col.lower()\n",
    "]\n",
    "\n",
    "humidity_col = [\n",
    "    col for col in merged_data.columns \n",
    "    if \"humidity\" in col.lower() and \"lorawan_readings\" in col.lower()\n",
    "]\n",
    "\n",
    "co2_col = [\n",
    "    col for col in merged_data.columns \n",
    "    if \"co2\" in col.lower() and \"lorawan_readings\" in col.lower()\n",
    "]\n",
    "\n",
    "sensors_to_keep = [\"Sensor_1\", \"Sensor_3\", \"Sensor_6\"]\n",
    "sensors_col = [col for col in merged_data.columns if any(sensor in col for sensor in sensors_to_keep)]\n",
    "\n",
    "weather_cols_to_keep = [\"weather_status\",\"weather_temp\",\"weather_humidity\"]\n",
    "weather_col = [col for col in merged_data.columns if any(weathercol in col for weathercol in weather_cols_to_keep)]\n",
    "\n",
    "#adding avg temp humid and co2\n",
    "merged_data['avg_temperature'] = merged_data[temperature_col].mean(axis=1)\n",
    "merged_data['avg_humidity'] = merged_data[humidity_col].mean(axis=1)\n",
    "merged_data['avg_co2'] = merged_data[co2_col].mean(axis=1)\n",
    "\n",
    "avg_col = [\n",
    "    col for col in merged_data.columns\n",
    "    if \"avg\" in col.lower()\n",
    "]\n",
    "\n",
    "#Energy(power,energy,current) Data\n",
    "energy_data = merged_data[[\"ISO_formatted_datetime\"]+ sensors_col]\n",
    "energy_data.columns = energy_data.columns.str.replace(\n",
    "    r\"Energy_Readings.Sensor_1\\.(Current|Energy|Power)\", \"compressor_\\\\1\", regex=True\n",
    ").str.replace(\n",
    "    r\"Energy_Readings.Sensor_3\\.(Current|Energy|Power)\", \"fancoil_1_\\\\1\", regex=True\n",
    ").str.replace(\n",
    "    r\"Energy_Readings.Sensor_6\\.(Current|Energy|Power)\", \"fancoil_2_\\\\1\", regex=True\n",
    ")\n",
    "\n",
    "#indoor Data\n",
    "indoor_data = merged_data[[\"ISO_formatted_datetime\"] + temperature_col + humidity_col + co2_col + avg_col]\n",
    "\n",
    "\n",
    "#Weather data\n",
    "weather_data = merged_data[[\"ISO_formatted_datetime\"]+ weather_col]\n",
    "weather_data.columns = weather_data.columns.str.replace(\n",
    "    r\"result.weather_status\", \"weather_status\", regex=True\n",
    ").str.replace(\n",
    "    r\"result.weather_temp\", \"weather_temp\", regex=True\n",
    ").str.replace(\n",
    "    r\"result.weather_humidity\", \"weather_humidity\", regex=True\n",
    ")\n",
    "\n",
    "\n",
    "#merging all the needed data\n",
    "energy_indoor_merged = pd.merge(energy_data, indoor_data, on='ISO_formatted_datetime', how='inner')\n",
    "final_merged_data = pd.merge(energy_indoor_merged, weather_data, on='ISO_formatted_datetime', how='inner')\n",
    "\n",
    "final_merged_data['total_energy'] = (\n",
    "    final_merged_data['compressor_Energy'] +\n",
    "    final_merged_data['fancoil_1_Energy'] +\n",
    "    final_merged_data['fancoil_2_Energy']\n",
    ")\n",
    "\n",
    "final_merged_data['total_power'] = (\n",
    "    final_merged_data['compressor_Power'] +\n",
    "    final_merged_data['fancoil_1_Power'] +\n",
    "    final_merged_data['fancoil_2_Power']\n",
    ")\n",
    "\n",
    "final_merged_data['total_current'] = (\n",
    "    final_merged_data['compressor_Current'] +\n",
    "    final_merged_data['fancoil_1_Current'] +\n",
    "    final_merged_data['fancoil_2_Current']\n",
    ")\n",
    "final_merged_data['hour'] = pd.to_datetime(final_merged_data['ISO_formatted_datetime']).dt.hour\n",
    "final_merged_data['day_of_week'] = pd.to_datetime(final_merged_data['ISO_formatted_datetime']).dt.dayofweek\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47a79a50-f406-499b-8963-707b4dc613c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_target_col(df):\n",
    "    comfort_condition = (\n",
    "        (df['avg_co2'] < 1000) &  # CO2 level\n",
    "        (40 <= df['avg_humidity']) & (df['avg_humidity'] <= 60)\n",
    "    )\n",
    "    energy_eff_condtion =(\n",
    "        (df['total_power'] < 10) & \n",
    "        (df['total_current'] < 13)\n",
    "    )\n",
    "\n",
    "    comfortable_temp = df.loc[comfort_condition & energy_eff_condtion, 'avg_temperature'].median()\n",
    "\n",
    "    power_weight = 1 - (df['total_power'] / df['total_power'].max())\n",
    "    current_weight = 1 - (df['total_current'] / df['total_current'].max())\n",
    "        \n",
    "    '''\n",
    "    Target temperature calculation:\n",
    "    1. If comfort conditions are met, use the actual temperature\n",
    "    2. Otherwise, calculate a weighted temperature considering:\n",
    "        70% of the comfortable mean temperature\n",
    "        30% of the actual temperature\n",
    "        Adjusted by power and current consumption weights\n",
    "    '''\n",
    "    target = np.where(\n",
    "        comfort_condition, \n",
    "        df['avg_temperature'], \n",
    "        0.7 * comfortable_temp + 0.3 * df['avg_temperature'] * (power_weight + current_weight) / 2\n",
    "    )\n",
    "    return target\n",
    "    \n",
    "     \n",
    "target = create_target_col(final_merged_data)\n",
    "final_merged_data['optimal_temp'] = target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a02a51b1-087a-48f8-8267-750595e56c4f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenj\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 629.5652 - mae: 25.0276 - val_loss: 421.4310 - val_mae: 20.3605\n",
      "Epoch 2/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 282.4111 - mae: 15.8668 - val_loss: 19.0154 - val_mae: 3.3670\n",
      "Epoch 3/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16.5476 - mae: 3.1077 - val_loss: 7.4874 - val_mae: 1.9551\n",
      "Epoch 4/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 7.1685 - mae: 1.8394 - val_loss: 5.4973 - val_mae: 1.5236\n",
      "Epoch 5/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 6.6237 - mae: 1.6387 - val_loss: 4.8039 - val_mae: 1.3301\n",
      "Epoch 6/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.8023 - mae: 1.3312 - val_loss: 4.1393 - val_mae: 1.1765\n",
      "Epoch 7/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 5.3079 - mae: 1.3282 - val_loss: 3.7442 - val_mae: 1.0982\n",
      "Epoch 8/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4.1846 - mae: 1.1630 - val_loss: 3.4248 - val_mae: 1.0426\n",
      "Epoch 9/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.7106 - mae: 1.0849 - val_loss: 3.5525 - val_mae: 1.0290\n",
      "Epoch 10/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4900 - mae: 1.0175 - val_loss: 3.1204 - val_mae: 0.9563\n",
      "Epoch 11/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.4643 - mae: 1.0257 - val_loss: 2.8562 - val_mae: 0.8638\n",
      "Epoch 12/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.1748 - mae: 0.9196 - val_loss: 2.5148 - val_mae: 0.8280\n",
      "Epoch 13/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6660 - mae: 0.8348 - val_loss: 2.4400 - val_mae: 0.7720\n",
      "Epoch 14/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.5140 - mae: 0.8084 - val_loss: 2.1833 - val_mae: 0.7244\n",
      "Epoch 15/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1593 - mae: 0.7103 - val_loss: 2.0584 - val_mae: 0.6974\n",
      "Epoch 16/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.1177 - mae: 0.7070 - val_loss: 1.8597 - val_mae: 0.6430\n",
      "Epoch 17/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.9079 - mae: 0.6318 - val_loss: 1.7459 - val_mae: 0.5980\n",
      "Epoch 18/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0747 - mae: 0.6643 - val_loss: 1.6714 - val_mae: 0.5651\n",
      "Epoch 19/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.7139 - mae: 0.5825 - val_loss: 1.6033 - val_mae: 0.5357\n",
      "Epoch 20/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7289 - mae: 0.5701 - val_loss: 1.4886 - val_mae: 0.5059\n",
      "Epoch 21/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4924 - mae: 0.4989 - val_loss: 1.4129 - val_mae: 0.4818\n",
      "Epoch 22/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5410 - mae: 0.5041 - val_loss: 1.4893 - val_mae: 0.4691\n",
      "Epoch 23/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4134 - mae: 0.4740 - val_loss: 1.2930 - val_mae: 0.4511\n",
      "Epoch 24/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2715 - mae: 0.4242 - val_loss: 1.2252 - val_mae: 0.4093\n",
      "Epoch 25/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.3117 - mae: 0.4358 - val_loss: 1.1878 - val_mae: 0.3953\n",
      "Epoch 26/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2417 - mae: 0.4002 - val_loss: 1.2056 - val_mae: 0.4009\n",
      "Epoch 27/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.1530 - mae: 0.3781 - val_loss: 1.1480 - val_mae: 0.3637\n",
      "Epoch 28/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0820 - mae: 0.3614 - val_loss: 1.0543 - val_mae: 0.3122\n",
      "Epoch 29/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0920 - mae: 0.3352 - val_loss: 1.0817 - val_mae: 0.3238\n",
      "Epoch 30/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0592 - mae: 0.3407 - val_loss: 1.0193 - val_mae: 0.3064\n",
      "Epoch 31/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0740 - mae: 0.3506 - val_loss: 1.0551 - val_mae: 0.3012\n",
      "Epoch 32/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0559 - mae: 0.3282 - val_loss: 0.9658 - val_mae: 0.2872\n",
      "Epoch 33/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9865 - mae: 0.2954 - val_loss: 0.9795 - val_mae: 0.2702\n",
      "Epoch 34/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9051 - mae: 0.2586 - val_loss: 0.9589 - val_mae: 0.2850\n",
      "Epoch 35/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8925 - mae: 0.2445 - val_loss: 0.9239 - val_mae: 0.2595\n",
      "Epoch 36/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8961 - mae: 0.2545 - val_loss: 1.0909 - val_mae: 0.3145\n",
      "Epoch 37/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9010 - mae: 0.2583 - val_loss: 1.0265 - val_mae: 0.2865\n",
      "Epoch 38/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8717 - mae: 0.2483 - val_loss: 0.8875 - val_mae: 0.2510\n",
      "Epoch 39/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8396 - mae: 0.2413 - val_loss: 0.8752 - val_mae: 0.2447\n",
      "Epoch 40/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8129 - mae: 0.2287 - val_loss: 0.8912 - val_mae: 0.2723\n",
      "Epoch 41/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8132 - mae: 0.2399 - val_loss: 0.8824 - val_mae: 0.2801\n",
      "Epoch 42/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7990 - mae: 0.2331 - val_loss: 0.8250 - val_mae: 0.2380\n",
      "Epoch 43/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7763 - mae: 0.2219 - val_loss: 0.7977 - val_mae: 0.2120\n",
      "Epoch 44/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8056 - mae: 0.2268 - val_loss: 0.8285 - val_mae: 0.2377\n",
      "Epoch 45/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7476 - mae: 0.2109 - val_loss: 0.7875 - val_mae: 0.2299\n",
      "Epoch 46/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7859 - mae: 0.2466 - val_loss: 0.7781 - val_mae: 0.2258\n",
      "Epoch 47/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7206 - mae: 0.2022 - val_loss: 0.8300 - val_mae: 0.2670\n",
      "Epoch 48/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7382 - mae: 0.2201 - val_loss: 0.7347 - val_mae: 0.2160\n",
      "Epoch 49/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6860 - mae: 0.1887 - val_loss: 0.7284 - val_mae: 0.2269\n",
      "Epoch 50/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7104 - mae: 0.2193 - val_loss: 0.7478 - val_mae: 0.2171\n",
      "Epoch 51/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7179 - mae: 0.2399 - val_loss: 0.7429 - val_mae: 0.2529\n",
      "Epoch 52/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6782 - mae: 0.2190 - val_loss: 0.6947 - val_mae: 0.2108\n",
      "Epoch 53/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6540 - mae: 0.2019 - val_loss: 0.7146 - val_mae: 0.2085\n",
      "Epoch 54/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6623 - mae: 0.2167 - val_loss: 0.6602 - val_mae: 0.1862\n",
      "Epoch 55/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6452 - mae: 0.2201 - val_loss: 0.7063 - val_mae: 0.2412\n",
      "Epoch 56/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6179 - mae: 0.2140 - val_loss: 0.6680 - val_mae: 0.2075\n",
      "Epoch 57/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5918 - mae: 0.1796 - val_loss: 0.7059 - val_mae: 0.2622\n",
      "Epoch 58/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6406 - mae: 0.2198 - val_loss: 0.6863 - val_mae: 0.2523\n",
      "Epoch 59/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6376 - mae: 0.2473 - val_loss: 0.6078 - val_mae: 0.2184\n",
      "Epoch 60/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6039 - mae: 0.2209 - val_loss: 0.5918 - val_mae: 0.1747\n",
      "Epoch 61/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6184 - mae: 0.2110 - val_loss: 0.5909 - val_mae: 0.2016\n",
      "Epoch 62/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5389 - mae: 0.1810 - val_loss: 0.6677 - val_mae: 0.2333\n",
      "Epoch 63/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5731 - mae: 0.2353 - val_loss: 0.5704 - val_mae: 0.1817\n",
      "Epoch 64/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5584 - mae: 0.2019 - val_loss: 0.5740 - val_mae: 0.1915\n",
      "Epoch 65/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5519 - mae: 0.1899 - val_loss: 0.5704 - val_mae: 0.2018\n",
      "Epoch 66/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5497 - mae: 0.2107 - val_loss: 0.5515 - val_mae: 0.2204\n",
      "Epoch 67/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5236 - mae: 0.2050 - val_loss: 0.6101 - val_mae: 0.2394\n",
      "Epoch 68/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5065 - mae: 0.1817 - val_loss: 0.5841 - val_mae: 0.2615\n",
      "Epoch 69/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4915 - mae: 0.1895 - val_loss: 0.5463 - val_mae: 0.2130\n",
      "Epoch 70/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5009 - mae: 0.2019 - val_loss: 0.5533 - val_mae: 0.2663\n",
      "Epoch 71/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4966 - mae: 0.2038 - val_loss: 0.4916 - val_mae: 0.1576\n",
      "Epoch 72/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5015 - mae: 0.1928 - val_loss: 0.5041 - val_mae: 0.2079\n",
      "Epoch 73/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4635 - mae: 0.2019 - val_loss: 0.5519 - val_mae: 0.2947\n",
      "Epoch 74/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4964 - mae: 0.2288 - val_loss: 0.4831 - val_mae: 0.1815\n",
      "Epoch 75/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4504 - mae: 0.1728 - val_loss: 0.5001 - val_mae: 0.2074\n",
      "Epoch 76/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4607 - mae: 0.1949 - val_loss: 0.5113 - val_mae: 0.2175\n",
      "Epoch 77/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4579 - mae: 0.2215 - val_loss: 0.4614 - val_mae: 0.1839\n",
      "Epoch 78/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4456 - mae: 0.1894 - val_loss: 0.4519 - val_mae: 0.1625\n",
      "Epoch 79/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.4457 - mae: 0.1974 - val_loss: 0.4677 - val_mae: 0.2154\n",
      "Epoch 80/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.4192 - mae: 0.1753 - val_loss: 0.4448 - val_mae: 0.1716\n",
      "Epoch 81/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.4021 - mae: 0.1822 - val_loss: 0.4366 - val_mae: 0.1767\n",
      "Epoch 82/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.4856 - mae: 0.2247 - val_loss: 0.5097 - val_mae: 0.2186\n",
      "Epoch 83/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4563 - mae: 0.2016 - val_loss: 0.4628 - val_mae: 0.1941\n",
      "Epoch 84/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4047 - mae: 0.1829 - val_loss: 0.4434 - val_mae: 0.1970\n",
      "Epoch 85/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4097 - mae: 0.1754 - val_loss: 0.4895 - val_mae: 0.3061\n",
      "Epoch 86/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4253 - mae: 0.2085 - val_loss: 0.4142 - val_mae: 0.1761\n",
      "Epoch 87/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3832 - mae: 0.1626 - val_loss: 0.4108 - val_mae: 0.1715\n",
      "Epoch 88/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3695 - mae: 0.1740 - val_loss: 0.4677 - val_mae: 0.2720\n",
      "Epoch 89/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3692 - mae: 0.1903 - val_loss: 0.4055 - val_mae: 0.1876\n",
      "Epoch 90/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3822 - mae: 0.1935 - val_loss: 0.4062 - val_mae: 0.2108\n",
      "Epoch 91/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4238 - mae: 0.2170 - val_loss: 0.3945 - val_mae: 0.1774\n",
      "Epoch 92/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3806 - mae: 0.1718 - val_loss: 0.4002 - val_mae: 0.1780\n",
      "Epoch 93/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3452 - mae: 0.1513 - val_loss: 0.3720 - val_mae: 0.1547\n",
      "Epoch 94/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3536 - mae: 0.1881 - val_loss: 0.4306 - val_mae: 0.3073\n",
      "Epoch 95/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3646 - mae: 0.2066 - val_loss: 0.3993 - val_mae: 0.1973\n",
      "Epoch 96/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3682 - mae: 0.1944 - val_loss: 0.5657 - val_mae: 0.2551\n",
      "Epoch 97/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4401 - mae: 0.2635 - val_loss: 0.3976 - val_mae: 0.2067\n",
      "Epoch 98/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3459 - mae: 0.1907 - val_loss: 0.3747 - val_mae: 0.1961\n",
      "Epoch 99/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3375 - mae: 0.1837 - val_loss: 0.3760 - val_mae: 0.1832\n",
      "Epoch 100/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3190 - mae: 0.1625 - val_loss: 0.3554 - val_mae: 0.1888\n",
      "Epoch 101/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3416 - mae: 0.2026 - val_loss: 0.3665 - val_mae: 0.2333\n",
      "Epoch 102/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3109 - mae: 0.1792 - val_loss: 0.4209 - val_mae: 0.1873\n",
      "Epoch 103/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3983 - mae: 0.2442 - val_loss: 0.3481 - val_mae: 0.1746\n",
      "Epoch 104/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3088 - mae: 0.1659 - val_loss: 0.4164 - val_mae: 0.2097\n",
      "Epoch 105/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3096 - mae: 0.1868 - val_loss: 0.3812 - val_mae: 0.2215\n",
      "Epoch 106/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3063 - mae: 0.1785 - val_loss: 0.3420 - val_mae: 0.1856\n",
      "Epoch 107/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3113 - mae: 0.1863 - val_loss: 0.3939 - val_mae: 0.2558\n",
      "Epoch 108/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3572 - mae: 0.2259 - val_loss: 0.3555 - val_mae: 0.1948\n",
      "Epoch 109/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3310 - mae: 0.2094 - val_loss: 0.3190 - val_mae: 0.1565\n",
      "Epoch 110/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2825 - mae: 0.1528 - val_loss: 0.3937 - val_mae: 0.2906\n",
      "Epoch 111/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3029 - mae: 0.1817 - val_loss: 0.3212 - val_mae: 0.1943\n",
      "Epoch 112/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2912 - mae: 0.1614 - val_loss: 0.3192 - val_mae: 0.1894\n",
      "Epoch 113/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3047 - mae: 0.1895 - val_loss: 0.3144 - val_mae: 0.1958\n",
      "Epoch 114/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2871 - mae: 0.1713 - val_loss: 0.3544 - val_mae: 0.1948\n",
      "Epoch 115/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2789 - mae: 0.1727 - val_loss: 0.3077 - val_mae: 0.1728\n",
      "Epoch 116/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3021 - mae: 0.1860 - val_loss: 0.3238 - val_mae: 0.2136\n",
      "Epoch 117/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2808 - mae: 0.1809 - val_loss: 0.3641 - val_mae: 0.2454\n",
      "Epoch 118/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2972 - mae: 0.2135 - val_loss: 0.3010 - val_mae: 0.2156\n",
      "Epoch 119/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2784 - mae: 0.1719 - val_loss: 0.3468 - val_mae: 0.2283\n",
      "Epoch 120/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2670 - mae: 0.1827 - val_loss: 0.3067 - val_mae: 0.1829\n",
      "Epoch 121/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2985 - mae: 0.1797 - val_loss: 0.3209 - val_mae: 0.2244\n",
      "Epoch 122/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2504 - mae: 0.1626 - val_loss: 0.2898 - val_mae: 0.1640\n",
      "Epoch 123/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2525 - mae: 0.1712 - val_loss: 0.4044 - val_mae: 0.2565\n",
      "Epoch 124/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2842 - mae: 0.1722 - val_loss: 0.2880 - val_mae: 0.1572\n",
      "Epoch 125/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2417 - mae: 0.1426 - val_loss: 0.2851 - val_mae: 0.1643\n",
      "Epoch 126/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2524 - mae: 0.1703 - val_loss: 0.3622 - val_mae: 0.2788\n",
      "Epoch 127/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2707 - mae: 0.2036 - val_loss: 0.2874 - val_mae: 0.1636\n",
      "Epoch 128/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2673 - mae: 0.1780 - val_loss: 0.3720 - val_mae: 0.2098\n",
      "Epoch 129/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2546 - mae: 0.1665 - val_loss: 0.3242 - val_mae: 0.2055\n",
      "Epoch 130/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2387 - mae: 0.1608 - val_loss: 0.2924 - val_mae: 0.1962\n",
      "Epoch 131/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2634 - mae: 0.1729 - val_loss: 0.2854 - val_mae: 0.2043\n",
      "Epoch 132/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2529 - mae: 0.1706 - val_loss: 0.2653 - val_mae: 0.1555\n",
      "Epoch 133/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2507 - mae: 0.1868 - val_loss: 0.2787 - val_mae: 0.1847\n",
      "Epoch 134/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2339 - mae: 0.1603 - val_loss: 0.3112 - val_mae: 0.2273\n",
      "Epoch 135/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2436 - mae: 0.1736 - val_loss: 0.2807 - val_mae: 0.2208\n",
      "Epoch 136/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2502 - mae: 0.1819 - val_loss: 0.2564 - val_mae: 0.1554\n",
      "Epoch 137/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2448 - mae: 0.1609 - val_loss: 0.2614 - val_mae: 0.1474\n",
      "Epoch 138/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2198 - mae: 0.1437 - val_loss: 0.2753 - val_mae: 0.1879\n",
      "Epoch 139/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2321 - mae: 0.1866 - val_loss: 0.2802 - val_mae: 0.2188\n",
      "Epoch 140/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2319 - mae: 0.1657 - val_loss: 0.3050 - val_mae: 0.1945\n",
      "Epoch 141/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2278 - mae: 0.1617 - val_loss: 0.2660 - val_mae: 0.1664\n",
      "Epoch 142/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2257 - mae: 0.1518 - val_loss: 0.2841 - val_mae: 0.1774\n",
      "Epoch 143/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2071 - mae: 0.1421 - val_loss: 0.2584 - val_mae: 0.1907\n",
      "Epoch 144/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2293 - mae: 0.1739 - val_loss: 0.2616 - val_mae: 0.1927\n",
      "Epoch 145/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3607 - mae: 0.2720 - val_loss: 0.3122 - val_mae: 0.2484\n",
      "Epoch 146/500\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4687 - mae: 0.3336 - val_loss: 0.2666 - val_mae: 0.1866\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3223 - mae: 0.1846 \n",
      "Mean Absolute Error: 0.18343131244182587\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,BatchNormalization, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "# Relevant columns for input\n",
    "input_features = [\n",
    "    'compressor_Current', 'compressor_Energy', 'compressor_Power',\n",
    "    'fancoil_1_Current', 'fancoil_1_Energy', 'fancoil_1_Power',\n",
    "    'fancoil_2_Current', 'fancoil_2_Energy', 'fancoil_2_Power',\n",
    "    'avg_temperature', 'avg_humidity', 'avg_co2',\n",
    "    'weather_temp', 'weather_humidity', 'total_energy', \n",
    "    'total_power', 'total_current', 'hour', 'day_of_week'\n",
    "]\n",
    "\n",
    "# Adding one-hot encoding for weather_status\n",
    "categorical_features = ['weather_status']\n",
    "\n",
    "# Preprocessing\n",
    "X = final_merged_data[input_features + categorical_features]\n",
    "y = final_merged_data['optimal_temp']  # Replace 'target_temp' with the actual column name for the target\n",
    "\n",
    "# Transform categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), input_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X = preprocessor.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Neural network\n",
    "model = Sequential([\n",
    "# removing dropout since our dataset very less\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(0.01), input_shape=(X_train.shape[1],)),\n",
    "    \n",
    "    Dense(16, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "\n",
    "#     BatchNormalization(),\n",
    "\n",
    "#     # Second layer\n",
    "#     Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "#     BatchNormalization(),\n",
    "\n",
    "#     # Third layer\n",
    "#     Dense(16, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "#     BatchNormalization(),\n",
    "\n",
    "    # Output layer\n",
    "    Dense(1, activation='linear')  # Predicting a continuous value\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=500, \n",
    "    batch_size=8, \n",
    "    validation_split=0.3, \n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Mean Absolute Error: {mae}\")#lower the better\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "model.save(\"models/optimal_temp_NN_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08abe507-124e-4f65-ba2e-6b4f018dbfa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract training history\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "train_mae = history.history['mae']\n",
    "val_mae = history.history['val_mae']\n",
    "\n",
    "# Plot the metrics\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Loss plot\n",
    "plt.figure(figsize=(24, 12))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss, label='Training Loss')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "\n",
    "# MAE plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_mae, label='Training MAE')\n",
    "plt.plot(epochs, val_mae, label='Validation MAE')\n",
    "plt.title('Mean Absolute Error (MAE) Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be6b15-0f94-48ac-a23e-eab94b533ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
