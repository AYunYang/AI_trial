{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc4ce40e-d027-4e27-bf59-c1e7e679c701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2438 entries, 0 to 2437\n",
      "Data columns (total 44 columns):\n",
      " #   Column                    Non-Null Count  Dtype         \n",
      "---  ------                    --------------  -----         \n",
      " 0   Datetime                  2438 non-null   datetime64[ns]\n",
      " 1   FC_Unit_1_Status          2438 non-null   object        \n",
      " 2   FC_Unit_1_Fan_Status      2438 non-null   object        \n",
      " 3   FC_Unit_1_Set_Point       2438 non-null   float64       \n",
      " 4   FC_Unit_1_Operation_Mode  2438 non-null   object        \n",
      " 5   FC_Unit_2_Status          2438 non-null   object        \n",
      " 6   FC_Unit_2_Fan_Status      2438 non-null   object        \n",
      " 7   FC_Unit_2_Set_Point       2438 non-null   float64       \n",
      " 8   FC_Unit_2_Operation_Mode  2438 non-null   object        \n",
      " 9   FC_Unit_3_Status          2438 non-null   object        \n",
      " 10  FC_Unit_3_Fan_Status      2438 non-null   object        \n",
      " 11  FC_Unit_3_Set_Point       2438 non-null   float64       \n",
      " 12  FC_Unit_3_Operation_Mode  2438 non-null   object        \n",
      " 13  FC_Unit_4_Status          2438 non-null   object        \n",
      " 14  FC_Unit_4_Fan_Status      2438 non-null   object        \n",
      " 15  FC_Unit_4_Set_Point       2438 non-null   float64       \n",
      " 16  FC_Unit_4_Operation_Mode  2438 non-null   object        \n",
      " 17  FC_Unit_5_Status          2438 non-null   object        \n",
      " 18  FC_Unit_5_Fan_Status      2438 non-null   object        \n",
      " 19  FC_Unit_5_Set_Point       2438 non-null   float64       \n",
      " 20  FC_Unit_5_Operation_Mode  2438 non-null   object        \n",
      " 21  FC_Unit_6_Status          2438 non-null   object        \n",
      " 22  FC_Unit_6_Fan_Status      2438 non-null   object        \n",
      " 23  FC_Unit_6_Set_Point       2438 non-null   float64       \n",
      " 24  FC_Unit_6_Operation_Mode  2438 non-null   object        \n",
      " 25  FC_Unit_7_Status          2438 non-null   object        \n",
      " 26  FC_Unit_7_Fan_Status      2438 non-null   object        \n",
      " 27  FC_Unit_7_Set_Point       2438 non-null   float64       \n",
      " 28  FC_Unit_7_Operation_Mode  2438 non-null   object        \n",
      " 29  FC_Unit_8_Status          2438 non-null   object        \n",
      " 30  FC_Unit_8_Fan_Status      2438 non-null   object        \n",
      " 31  FC_Unit_8_Set_Point       2438 non-null   float64       \n",
      " 32  FC_Unit_8_Operation_Mode  2438 non-null   object        \n",
      " 33  weather_status            2438 non-null   object        \n",
      " 34  weather_temp              2438 non-null   float64       \n",
      " 35  weather_humid             2438 non-null   int64         \n",
      " 36  total_energy              2438 non-null   float64       \n",
      " 37  total_power               2438 non-null   float64       \n",
      " 38  total_current             2438 non-null   float64       \n",
      " 39  avg_temperature           2438 non-null   float64       \n",
      " 40  avg_humidity              2438 non-null   float64       \n",
      " 41  avg_co2                   2438 non-null   float64       \n",
      " 42  hour                      2438 non-null   int32         \n",
      " 43  Day_of_week               2438 non-null   int32         \n",
      "dtypes: datetime64[ns](1), float64(15), int32(2), int64(1), object(25)\n",
      "memory usage: 819.1+ KB\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "Sensor_readings = pd.read_json('data/json/W512.w512_readings (1).json')\n",
    "Aircon_Data = pd.read_json('data/json/W512.w512_aircon_status (1).json')\n",
    "Weather_readings = pd.read_json('data/json/user.weather_data (1).json')\n",
    "\n",
    "def convert_AirconData(data):\n",
    "    records = []\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        # Parse FC_FullStatus_Readings if it's a string representation of a dictionary\n",
    "        if isinstance(row['FC_FullStatus_Readings'], str):\n",
    "            fc_readings = ast.literal_eval(row['FC_FullStatus_Readings'])\n",
    "        else:\n",
    "            fc_readings = row['FC_FullStatus_Readings']\n",
    "\n",
    "\n",
    "        try:\n",
    "            combined_datetime = pd.to_datetime(f\"{row['date']} {row['time']}\")\n",
    "            formatted_datetime = pd.to_datetime(combined_datetime.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error combining datetime for row {index}: {e}\")\n",
    "            combined_datetime = None\n",
    "            formatted_datetime = None\n",
    "\n",
    "        \n",
    "        # Create a record with base information\n",
    "        record = {\n",
    "            'Datetime': formatted_datetime\n",
    "        }\n",
    "        \n",
    "        # Add each FC Unit's details as separate columns\n",
    "        for unit, unit_data in fc_readings.items():\n",
    "            record[f'{unit}_Status'] = unit_data['Status']\n",
    "            record[f'{unit}_Fan_Status'] = unit_data['Fan_Status']\n",
    "            record[f'{unit}_Set_Point'] = unit_data['Set_Point']\n",
    "            record[f'{unit}_Operation_Mode'] = unit_data['Operation_Mode']\n",
    "        \n",
    "        records.append(record)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_sensorReadings(data):\n",
    "    records = []\n",
    "    \n",
    "    # List of keys to exclude from Lorawan_Readings\n",
    "    include_keys_1 = [\"24E124725E285123\", \"24E124725E331695\",\"24E124725E331744\",\n",
    "                      \"24E124725E332483\",\"24E124725E290348\",\"24E124725E331733\",\"24E124725E286745\"]#\"24E124136D316361\" is suppiosed to be outdoor but it is not outdoor yet\n",
    "    include_keys_2 = [\"Sensor_1\",\"Sensor_3\",\"Sensor_6\"]\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        # Parse Energy_Readings if it's a string representation of a dictionary\n",
    "        if isinstance(row['Energy_Readings'], str):\n",
    "            Energy_readings = ast.literal_eval(row['Energy_Readings'])\n",
    "        else:\n",
    "            Energy_readings = row['Energy_Readings']\n",
    "            \n",
    "        # Parse Lorawan_Readings if it's a string representation of a dictionary\n",
    "        if isinstance(row['Lorawan_Readings'], str):\n",
    "            Lorawan_Readings = ast.literal_eval(row['Lorawan_Readings'])\n",
    "        else:\n",
    "            Lorawan_Readings = row['Lorawan_Readings']\n",
    "\n",
    "        try:\n",
    "            # Combine the date and time columns to create a datetime object\n",
    "            combined_datetime = pd.to_datetime(f\"{row['date']} {row['time']}\")\n",
    "            formatted_datetime = pd.to_datetime(combined_datetime.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Error combining datetime for row {index}: {e}\")\n",
    "            formatted_datetime = None\n",
    "\n",
    "        # Create a record with base information\n",
    "        record = {\n",
    "            'Datetime': formatted_datetime\n",
    "        }\n",
    "        \n",
    "        # Add each Energy sensor's details as separate columns\n",
    "        for unit, unit_data in Energy_readings.items():\n",
    "            if unit not in include_keys_2:\n",
    "                continue\n",
    "                \n",
    "            record[f'{unit}_Current'] = unit_data['Current']\n",
    "            record[f'{unit}_Energy'] = unit_data['Energy']\n",
    "            record[f'{unit}_Power'] = unit_data['Power']\n",
    "        \n",
    "        # Add each Lorawan device's details as separate columns\n",
    "        for unit, unit_data in Lorawan_Readings.items():\n",
    "            if unit not in include_keys_1:\n",
    "                continue\n",
    "            record[f'{unit}_Humidity'] = unit_data.get('humidity', None)\n",
    "            record[f'{unit}_Temperature'] = unit_data.get('temperature', None)\n",
    "\n",
    "            co2_value = unit_data.get('co2', None)\n",
    "            if co2_value is not None:\n",
    "                record[f'{unit}_CO2'] = co2_value\n",
    "\n",
    "        # Append the record to the list of records\n",
    "        records.append(record)\n",
    "    df=pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_weatherData(data):\n",
    "    records = []\n",
    "    for index, row in data.iterrows():\n",
    "        # Parse Energy_Readings if it's a string representation of a dictionary\n",
    "        if isinstance(row['result'], str):\n",
    "            weather_results = ast.literal_eval(row['result'])\n",
    "        else:\n",
    "            weather_results = row['result']\n",
    "            \n",
    "        try:\n",
    "            combined_datetime = pd.to_datetime(f\"{row['date']} {row['time']}\")\n",
    "            formatted_datetime = pd.to_datetime(combined_datetime.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Error combining datetime for row {index}: {e}\")\n",
    "            formatted_datetime = None\n",
    "\n",
    "        record = {\n",
    "            'Datetime': formatted_datetime\n",
    "        }  \n",
    "\n",
    "        record['weather_status'] = weather_results['weather_status']\n",
    "        record['weather_temp'] = weather_results['weather_temp']\n",
    "        record['weather_humid'] = weather_results['weather_humidity']\n",
    "            \n",
    "        records.append(record)\n",
    "    df=pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "Aircon_data_df = convert_AirconData(Aircon_Data)\n",
    "Aircon_data_df = Aircon_data_df[3194:]\n",
    "Sensor_readings_df = convert_sensorReadings(Sensor_readings)\n",
    "Sensor_readings_df = Sensor_readings_df.interpolate(method='linear')\n",
    "weather_readings_df = convert_weatherData(Weather_readings)\n",
    "\n",
    "# Merge Aircon data with Sensor readings using merge_asof\n",
    "merged_df = pd.merge_asof(Aircon_data_df, Sensor_readings_df, on='Datetime', direction='nearest')\n",
    "\n",
    "# Now, merge the Weather readings with the previous result using merge_asof\n",
    "merged_df = pd.merge_asof(merged_df, weather_readings_df, on='Datetime', direction='nearest')\n",
    "\n",
    "\n",
    "merged_df['total_energy'] = (\n",
    "    merged_df['Sensor_1_Energy'] +\n",
    "    merged_df['Sensor_3_Energy'] +\n",
    "    merged_df['Sensor_6_Energy']\n",
    ")\n",
    "\n",
    "merged_df['total_power'] = (\n",
    "    merged_df['Sensor_1_Power'] +\n",
    "    merged_df['Sensor_3_Power'] +\n",
    "    merged_df['Sensor_6_Power']\n",
    ")\n",
    "\n",
    "merged_df['total_current'] = (\n",
    "    merged_df['Sensor_1_Current'] +\n",
    "    merged_df['Sensor_3_Current'] +\n",
    "    merged_df['Sensor_6_Current']\n",
    ")\n",
    "\n",
    "temperature_col = [\n",
    "    col for col in merged_df.columns \n",
    "    if \"24e124\" in col.lower() and \"temperature\" in col.lower()\n",
    "]\n",
    "humidity_col = [\n",
    "    col for col in merged_df.columns \n",
    "    if \"24e124\" in col.lower() and \"humidity\" in col.lower()\n",
    "]\n",
    "co2_col = [\n",
    "    col for col in merged_df.columns \n",
    "    if \"24e124\" in col.lower() and \"co2\" in col.lower()\n",
    "]\n",
    "\n",
    "merged_df['avg_temperature'] = merged_df[temperature_col].mean(axis=1)\n",
    "merged_df['avg_humidity'] = merged_df[humidity_col].mean(axis=1)\n",
    "merged_df['avg_co2'] = merged_df[co2_col].mean(axis=1)\n",
    "\n",
    "merged_df['hour'] = pd.to_datetime(merged_df['Datetime']).dt.hour\n",
    "merged_df['Day_of_week'] = pd.to_datetime(merged_df['Datetime']).dt.dayofweek\n",
    "\n",
    "dropped_col = [\n",
    "    col for col in merged_df.columns\n",
    "    if \"24e124\" in col.lower()\n",
    "]\n",
    "dropped_col_sensor = [\n",
    "    col for col in merged_df.columns\n",
    "    if \"sensor\" in col.lower()\n",
    "]\n",
    "\n",
    "columns_to_drop = dropped_col + dropped_col_sensor\n",
    "merged_df.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "merged_df.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df641a8b-76ef-4b0c-9ac4-6a9d0d963e26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.optimizers import Adam\\\n",
    "\n",
    "# Input and categorical features\n",
    "input_features = [\n",
    "    'avg_temperature', 'avg_humidity', 'avg_co2',\n",
    "    'weather_temp', 'weather_humid', 'total_energy',\n",
    "    'total_power', 'total_current', 'hour', 'Day_of_week'\n",
    "]\n",
    "cat_features = ['weather_status']\n",
    "\n",
    "# Fan unit columns\n",
    "fan_unit_col = [col for col in merged_df.columns if \"fc_unit\" in col.lower()]\n",
    "\n",
    "# Split X and y\n",
    "X = merged_df[input_features + cat_features]\n",
    "y = merged_df[fan_unit_col]\n",
    "\n",
    "# Preprocessor for X\n",
    "preprocessor_X = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), input_features),\n",
    "        ('cat', OneHotEncoder(), cat_features)\n",
    "    ]\n",
    ")\n",
    "X_processed = preprocessor_X.fit_transform(X)\n",
    "\n",
    "# Preprocessor for y\n",
    "cat_cols = [col for col in fan_unit_col if \"Status\" in col or \"Mode\" in col]\n",
    "num_cols = [col for col in fan_unit_col if \"Set_Point\" in col]\n",
    "preprocessor_y = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), cat_cols),\n",
    "        ('num', StandardScaler(), num_cols)\n",
    "    ]\n",
    ")\n",
    "y_processed = preprocessor_y.fit_transform(y)\n",
    "\n",
    "# Ensure y_processed is dense\n",
    "if hasattr(y_processed, \"toarray\"):\n",
    "    y_processed = y_processed.toarray()\n",
    "\n",
    "# Reshape X and y for LSTM\n",
    "X_processed = X_processed.reshape((X_processed.shape[0], 1, X_processed.shape[1]))\n",
    "y_processed = y_processed.reshape((y_processed.shape[0], 1, y_processed.shape[1]))\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_processed, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Define the LSTM model-building function for KerasTuner\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # LSTM layer\n",
    "    model.add(LSTM(\n",
    "        units=hp.Int('units_lstm', min_value=32, max_value=128, step=32),\n",
    "        activation='tanh',\n",
    "        recurrent_activation='sigmoid',\n",
    "        kernel_regularizer=l2(hp.Float('l2_lstm', 0.001, 0.1, step=0.001)),\n",
    "        input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "        return_sequences=hp.Boolean('return_sequences')\n",
    "    ))\n",
    "    model.add(Dropout(rate=hp.Float('dropout_lstm', 0.0, 0.5, step=0.1)))\n",
    "\n",
    "    # Dense layers\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('units_dense', min_value=16, max_value=64, step=16),\n",
    "        activation='relu',\n",
    "        kernel_regularizer=l2(hp.Float('l2_dense', 0.001, 0.1, step=0.001))\n",
    "    ))\n",
    "\n",
    "\n",
    "        # Output layer (multi-output)\n",
    "    model.add(Dense(y_train.shape[2], activation='linear'))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = Adam(learning_rate=hp.Choice('learning_rate', values=[1e-3, 1e-4]))   \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Initialize KerasTuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=100,\n",
    "    executions_per_trial=2,\n",
    "    directory='hyperparam_tuning',\n",
    "    project_name='fanunit_LSTM_1'\n",
    ")\n",
    "\n",
    "# Callback for early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    epochs=300,\n",
    "    validation_split=0.3,\n",
    "    batch_size=8,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_hps.values)\n",
    "\n",
    "# Build and train the model with best hyperparameters\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "history = best_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.3,\n",
    "    epochs=300,\n",
    "    batch_size=8,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "best_model.save('models/LSTM_Fanunit_v1.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3f144c-86b0-48e6-aff7-9baa4ac72f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Or ignore specific warning types\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# List of possible weather statuses\n",
    "weather_statuses = ['Clouds', 'Rain', 'Thunderstorm']\n",
    "\n",
    "# Relevant columns for input\n",
    "input_features = [\n",
    "    'avg_temperature', 'avg_humidity', 'avg_co2',\n",
    "    'weather_temp', 'weather_humid', 'total_energy', \n",
    "    'total_power', 'total_current', 'hour', 'Day_of_week'\n",
    "]\n",
    "categorical_features = ['weather_status']\n",
    "\n",
    "\n",
    "def generate_random_conditions():\n",
    "    \"\"\"Generate a dictionary of randomized conditions\"\"\"\n",
    "    return {\n",
    "        'ISO_formatted_datetime': datetime.now().isoformat(),\n",
    "        'avg_temperature': round(random.uniform(20, 30), 2),\n",
    "        'avg_humidity': round(random.uniform(30, 80), 2),\n",
    "        'avg_co2': round(random.uniform(350, 500), 2),\n",
    "        'weather_temp': round(random.uniform(22, 33), 2),\n",
    "        'weather_humid': round(random.uniform(40, 90), 2),\n",
    "        'total_current': round(random.uniform(0.5, 1.5), 2),\n",
    "        'total_energy': round(random.uniform(2000, 5000), 2),\n",
    "        'total_power': round(random.uniform(5, 15), 2),\n",
    "        'hour': random.randint(0, 23), \n",
    "        'Day_of_week': random.randint(0, 6),  \n",
    "        'weather_status': random.choice(weather_statuses)\n",
    "    }\n",
    "\n",
    "# Generate multiple random condition sets\n",
    "num_scenarios = 1\n",
    "scenario_results = []\n",
    "\n",
    "loaded_model = keras.models.load_model(\"models/LSTM_Fanunit_v1.keras\")\n",
    "\n",
    "for scenario in range(num_scenarios):\n",
    "    # Generate a random set of conditions\n",
    "    current_conditions = generate_random_conditions()\n",
    "    print(f\"\\nScenario {scenario + 1} Conditions:\")\n",
    "    for key, value in current_conditions.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    # Create a DataFrame with the current conditions\n",
    "    df = pd.DataFrame([current_conditions])\n",
    "    \n",
    "    # Preprocess\n",
    "    X_temp = preprocessor_X.transform(df[input_features + categorical_features])\n",
    "    X_temp = X_temp.reshape((X_temp.shape[0], 1, X_temp.shape[1]))\n",
    "    \n",
    "    # Predict\n",
    "    prediction = loaded_model.predict(X_temp)[0][0]\n",
    "    \n",
    "    # Print prediction\n",
    "    print(f\"{prediction}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ab2096-b0e4-4a95-be8e-d21003000115",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
