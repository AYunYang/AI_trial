{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee1758e8-a3df-4a64-9d71-f6fcf504e296",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2438 entries, 0 to 2437\n",
      "Data columns (total 69 columns):\n",
      " #   Column                        Non-Null Count  Dtype         \n",
      "---  ------                        --------------  -----         \n",
      " 0   Datetime                      2438 non-null   datetime64[ns]\n",
      " 1   FC_Unit_1_Status              2438 non-null   object        \n",
      " 2   FC_Unit_1_Fan_Status          2438 non-null   object        \n",
      " 3   FC_Unit_1_Set_Point           2438 non-null   float64       \n",
      " 4   FC_Unit_1_Operation_Mode      2438 non-null   object        \n",
      " 5   FC_Unit_2_Status              2438 non-null   object        \n",
      " 6   FC_Unit_2_Fan_Status          2438 non-null   object        \n",
      " 7   FC_Unit_2_Set_Point           2438 non-null   float64       \n",
      " 8   FC_Unit_2_Operation_Mode      2438 non-null   object        \n",
      " 9   FC_Unit_3_Status              2438 non-null   object        \n",
      " 10  FC_Unit_3_Fan_Status          2438 non-null   object        \n",
      " 11  FC_Unit_3_Set_Point           2438 non-null   float64       \n",
      " 12  FC_Unit_3_Operation_Mode      2438 non-null   object        \n",
      " 13  FC_Unit_4_Status              2438 non-null   object        \n",
      " 14  FC_Unit_4_Fan_Status          2438 non-null   object        \n",
      " 15  FC_Unit_4_Set_Point           2438 non-null   float64       \n",
      " 16  FC_Unit_4_Operation_Mode      2438 non-null   object        \n",
      " 17  FC_Unit_5_Status              2438 non-null   object        \n",
      " 18  FC_Unit_5_Fan_Status          2438 non-null   object        \n",
      " 19  FC_Unit_5_Set_Point           2438 non-null   float64       \n",
      " 20  FC_Unit_5_Operation_Mode      2438 non-null   object        \n",
      " 21  FC_Unit_6_Status              2438 non-null   object        \n",
      " 22  FC_Unit_6_Fan_Status          2438 non-null   object        \n",
      " 23  FC_Unit_6_Set_Point           2438 non-null   float64       \n",
      " 24  FC_Unit_6_Operation_Mode      2438 non-null   object        \n",
      " 25  FC_Unit_7_Status              2438 non-null   object        \n",
      " 26  FC_Unit_7_Fan_Status          2438 non-null   object        \n",
      " 27  FC_Unit_7_Set_Point           2438 non-null   float64       \n",
      " 28  FC_Unit_7_Operation_Mode      2438 non-null   object        \n",
      " 29  FC_Unit_8_Status              2438 non-null   object        \n",
      " 30  FC_Unit_8_Fan_Status          2438 non-null   object        \n",
      " 31  FC_Unit_8_Set_Point           2438 non-null   float64       \n",
      " 32  FC_Unit_8_Operation_Mode      2438 non-null   object        \n",
      " 33  Sensor_1_Current              2438 non-null   float64       \n",
      " 34  Sensor_1_Energy               2438 non-null   float64       \n",
      " 35  Sensor_1_Power                2438 non-null   float64       \n",
      " 36  Sensor_3_Current              2438 non-null   float64       \n",
      " 37  Sensor_3_Energy               2438 non-null   float64       \n",
      " 38  Sensor_3_Power                2438 non-null   float64       \n",
      " 39  Sensor_6_Current              2438 non-null   float64       \n",
      " 40  Sensor_6_Energy               2438 non-null   float64       \n",
      " 41  Sensor_6_Power                2438 non-null   float64       \n",
      " 42  24E124725E331695_Humidity     2438 non-null   float64       \n",
      " 43  24E124725E331695_Temperature  2438 non-null   float64       \n",
      " 44  24E124725E331744_Humidity     2438 non-null   float64       \n",
      " 45  24E124725E331744_Temperature  2438 non-null   float64       \n",
      " 46  24E124725E332483_Humidity     2438 non-null   float64       \n",
      " 47  24E124725E332483_Temperature  2438 non-null   float64       \n",
      " 48  24E124725E331733_Humidity     2438 non-null   float64       \n",
      " 49  24E124725E331733_Temperature  2438 non-null   float64       \n",
      " 50  24E124725E290348_Humidity     2438 non-null   float64       \n",
      " 51  24E124725E290348_Temperature  2438 non-null   float64       \n",
      " 52  24E124725E286745_Humidity     2438 non-null   float64       \n",
      " 53  24E124725E286745_Temperature  2438 non-null   float64       \n",
      " 54  24E124725E286745_CO2          2438 non-null   float64       \n",
      " 55  24E124725E285123_Humidity     2438 non-null   float64       \n",
      " 56  24E124725E285123_Temperature  2438 non-null   float64       \n",
      " 57  24E124725E285123_CO2          2438 non-null   float64       \n",
      " 58  weather_status                2438 non-null   object        \n",
      " 59  weather_temp                  2438 non-null   float64       \n",
      " 60  weather_humid                 2438 non-null   int64         \n",
      " 61  total_energy                  2438 non-null   float64       \n",
      " 62  total_power                   2438 non-null   float64       \n",
      " 63  total_current                 2438 non-null   float64       \n",
      " 64  avg_temperature               2438 non-null   float64       \n",
      " 65  avg_humidity                  2438 non-null   float64       \n",
      " 66  avg_co2                       2438 non-null   float64       \n",
      " 67  hour                          2438 non-null   int32         \n",
      " 68  Day_of_week                   2438 non-null   int32         \n",
      "dtypes: datetime64[ns](1), float64(40), int32(2), int64(1), object(25)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "Sensor_readings = pd.read_json('data/json/W512.w512_readings (1).json')\n",
    "Aircon_Data = pd.read_json('data/json/W512.w512_aircon_status (1).json')\n",
    "Weather_readings = pd.read_json('data/json/user.weather_data (1).json')\n",
    "\n",
    "def convert_AirconData(data):\n",
    "    records = []\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        # Parse FC_FullStatus_Readings if it's a string representation of a dictionary\n",
    "        if isinstance(row['FC_FullStatus_Readings'], str):\n",
    "            fc_readings = ast.literal_eval(row['FC_FullStatus_Readings'])\n",
    "        else:\n",
    "            fc_readings = row['FC_FullStatus_Readings']\n",
    "\n",
    "\n",
    "        try:\n",
    "            combined_datetime = pd.to_datetime(f\"{row['date']} {row['time']}\")\n",
    "            formatted_datetime = pd.to_datetime(combined_datetime.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error combining datetime for row {index}: {e}\")\n",
    "            combined_datetime = None\n",
    "            formatted_datetime = None\n",
    "\n",
    "        \n",
    "        # Create a record with base information\n",
    "        record = {\n",
    "            'Datetime': formatted_datetime\n",
    "        }\n",
    "        \n",
    "        # Add each FC Unit's details as separate columns\n",
    "        for unit, unit_data in fc_readings.items():\n",
    "            record[f'{unit}_Status'] = unit_data['Status']\n",
    "            record[f'{unit}_Fan_Status'] = unit_data['Fan_Status']\n",
    "            record[f'{unit}_Set_Point'] = unit_data['Set_Point']\n",
    "            record[f'{unit}_Operation_Mode'] = unit_data['Operation_Mode']\n",
    "        \n",
    "        records.append(record)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_sensorReadings(data):\n",
    "    records = []\n",
    "    \n",
    "    # List of keys to exclude from Lorawan_Readings\n",
    "    include_keys_1 = [\"24E124725E285123\", \"24E124725E331695\",\"24E124725E331744\",\n",
    "                      \"24E124725E332483\",\"24E124725E290348\",\"24E124725E331733\",\"24E124725E286745\"]#\"24E124136D316361\" is suppiosed to be outdoor but it is not outdoor yet\n",
    "    include_keys_2 = [\"Sensor_1\",\"Sensor_3\",\"Sensor_6\"]\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        # Parse Energy_Readings if it's a string representation of a dictionary\n",
    "        if isinstance(row['Energy_Readings'], str):\n",
    "            Energy_readings = ast.literal_eval(row['Energy_Readings'])\n",
    "        else:\n",
    "            Energy_readings = row['Energy_Readings']\n",
    "            \n",
    "        # Parse Lorawan_Readings if it's a string representation of a dictionary\n",
    "        if isinstance(row['Lorawan_Readings'], str):\n",
    "            Lorawan_Readings = ast.literal_eval(row['Lorawan_Readings'])\n",
    "        else:\n",
    "            Lorawan_Readings = row['Lorawan_Readings']\n",
    "\n",
    "        try:\n",
    "            # Combine the date and time columns to create a datetime object\n",
    "            combined_datetime = pd.to_datetime(f\"{row['date']} {row['time']}\")\n",
    "            formatted_datetime = pd.to_datetime(combined_datetime.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Error combining datetime for row {index}: {e}\")\n",
    "            formatted_datetime = None\n",
    "\n",
    "        # Create a record with base information\n",
    "        record = {\n",
    "            'Datetime': formatted_datetime\n",
    "        }\n",
    "        \n",
    "        # Add each Energy sensor's details as separate columns\n",
    "        for unit, unit_data in Energy_readings.items():\n",
    "            if unit not in include_keys_2:\n",
    "                continue\n",
    "                \n",
    "            record[f'{unit}_Current'] = unit_data['Current']\n",
    "            record[f'{unit}_Energy'] = unit_data['Energy']\n",
    "            record[f'{unit}_Power'] = unit_data['Power']\n",
    "        \n",
    "        # Add each Lorawan device's details as separate columns\n",
    "        for unit, unit_data in Lorawan_Readings.items():\n",
    "            if unit not in include_keys_1:\n",
    "                continue\n",
    "            record[f'{unit}_Humidity'] = unit_data.get('humidity', None)\n",
    "            record[f'{unit}_Temperature'] = unit_data.get('temperature', None)\n",
    "\n",
    "            co2_value = unit_data.get('co2', None)\n",
    "            if co2_value is not None:\n",
    "                record[f'{unit}_CO2'] = co2_value\n",
    "\n",
    "        # Append the record to the list of records\n",
    "        records.append(record)\n",
    "    df=pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_weatherData(data):\n",
    "    records = []\n",
    "    for index, row in data.iterrows():\n",
    "        # Parse Energy_Readings if it's a string representation of a dictionary\n",
    "        if isinstance(row['result'], str):\n",
    "            weather_results = ast.literal_eval(row['result'])\n",
    "        else:\n",
    "            weather_results = row['result']\n",
    "            \n",
    "        try:\n",
    "            combined_datetime = pd.to_datetime(f\"{row['date']} {row['time']}\")\n",
    "            formatted_datetime = pd.to_datetime(combined_datetime.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Error combining datetime for row {index}: {e}\")\n",
    "            formatted_datetime = None\n",
    "\n",
    "        record = {\n",
    "            'Datetime': formatted_datetime\n",
    "        }  \n",
    "\n",
    "        record['weather_status'] = weather_results['weather_status']\n",
    "        record['weather_temp'] = weather_results['weather_temp']\n",
    "        record['weather_humid'] = weather_results['weather_humidity']\n",
    "            \n",
    "        records.append(record)\n",
    "    df=pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "Aircon_data_df = convert_AirconData(Aircon_Data)\n",
    "Aircon_data_df = Aircon_data_df[3194:]\n",
    "Sensor_readings_df = convert_sensorReadings(Sensor_readings)\n",
    "Sensor_readings_df = Sensor_readings_df.interpolate(method='linear')\n",
    "weather_readings_df = convert_weatherData(Weather_readings)\n",
    "\n",
    "# Merge Aircon data with Sensor readings using merge_asof\n",
    "merged_df = pd.merge_asof(Aircon_data_df, Sensor_readings_df, on='Datetime', direction='nearest')\n",
    "\n",
    "# Now, merge the Weather readings with the previous result using merge_asof\n",
    "merged_df = pd.merge_asof(merged_df, weather_readings_df, on='Datetime', direction='nearest')\n",
    "\n",
    "\n",
    "merged_df['total_energy'] = (\n",
    "    merged_df['Sensor_1_Energy'] +\n",
    "    merged_df['Sensor_3_Energy'] +\n",
    "    merged_df['Sensor_6_Energy']\n",
    ")\n",
    "\n",
    "merged_df['total_power'] = (\n",
    "    merged_df['Sensor_1_Power'] +\n",
    "    merged_df['Sensor_3_Power'] +\n",
    "    merged_df['Sensor_6_Power']\n",
    ")\n",
    "\n",
    "merged_df['total_current'] = (\n",
    "    merged_df['Sensor_1_Current'] +\n",
    "    merged_df['Sensor_3_Current'] +\n",
    "    merged_df['Sensor_6_Current']\n",
    ")\n",
    "\n",
    "temperature_col = [\n",
    "    col for col in merged_df.columns \n",
    "    if \"24e124\" in col.lower() and \"temperature\" in col.lower()\n",
    "]\n",
    "humidity_col = [\n",
    "    col for col in merged_df.columns \n",
    "    if \"24e124\" in col.lower() and \"humidity\" in col.lower()\n",
    "]\n",
    "co2_col = [\n",
    "    col for col in merged_df.columns \n",
    "    if \"24e124\" in col.lower() and \"co2\" in col.lower()\n",
    "]\n",
    "\n",
    "merged_df['avg_temperature'] = merged_df[temperature_col].mean(axis=1)\n",
    "merged_df['avg_humidity'] = merged_df[humidity_col].mean(axis=1)\n",
    "merged_df['avg_co2'] = merged_df[co2_col].mean(axis=1)\n",
    "\n",
    "merged_df['hour'] = pd.to_datetime(merged_df['Datetime']).dt.hour\n",
    "merged_df['Day_of_week'] = pd.to_datetime(merged_df['Datetime']).dt.dayofweek\n",
    "\n",
    "merged_df.info()\n",
    "\n",
    "merged_df.to_csv('data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c2b2f58-79b9-489d-98b6-9a8de3881554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yy\\anaconda3\\envs\\yy_1\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([8, 56])) that is different to the input size (torch.Size([8, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (56) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 127\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m train_model(model, train_loader, optimizer, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m    130\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/FanUnitLSTM.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 118\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, optimizer, epochs)\u001b[0m\n\u001b[0;32m    115\u001b[0m pred_categorical, pred_numerical \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[0;32m    117\u001b[0m loss_categorical \u001b[38;5;241m=\u001b[39m categorical_loss_fn(pred_categorical, y_categorical\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 118\u001b[0m loss_numerical \u001b[38;5;241m=\u001b[39m numerical_loss_fn(pred_numerical, y_numerical)\n\u001b[0;32m    120\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_categorical \u001b[38;5;241m+\u001b[39m loss_numerical\n\u001b[0;32m    121\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yy_1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yy_1\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yy_1\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mmse_loss(\u001b[38;5;28minput\u001b[39m, target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yy_1\\Lib\\site-packages\\torch\\nn\\functional.py:3791\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3789\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3791\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(\n\u001b[0;32m   3793\u001b[0m     expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3794\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yy_1\\Lib\\site-packages\\torch\\functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mbroadcast_tensors(tensors)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (56) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Input and categorical features\n",
    "input_features = [\n",
    "    'avg_temperature', 'avg_humidity', 'avg_co2',\n",
    "    'weather_temp', 'weather_humid', 'total_energy',\n",
    "    'total_power', 'total_current', 'hour', 'Day_of_week'\n",
    "]\n",
    "cat_features = ['weather_status']\n",
    "\n",
    "# Fan unit columns\n",
    "fan_unit_col = [col for col in merged_df.columns if \"fc_unit\" in col.lower()]\n",
    "\n",
    "# Split X and y\n",
    "X = merged_df[input_features + cat_features]\n",
    "y = merged_df[fan_unit_col]\n",
    "\n",
    "# Preprocessor for X\n",
    "preprocessor_X = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), input_features),\n",
    "        ('cat', OneHotEncoder(), cat_features)\n",
    "    ]\n",
    ")\n",
    "X_processed = preprocessor_X.fit_transform(X)\n",
    "\n",
    "# Preprocessor for y\n",
    "cat_cols = [col for col in fan_unit_col if \"Status\" in col or \"Mode\" in col]\n",
    "num_cols = [col for col in fan_unit_col if \"Set_Point\" in col]\n",
    "preprocessor_y = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), cat_cols),\n",
    "        ('num', StandardScaler(), num_cols)\n",
    "    ]\n",
    ")\n",
    "y_processed = preprocessor_y.fit_transform(y)\n",
    "\n",
    "\n",
    "\n",
    "# Ensure y_processed is dense\n",
    "if hasattr(y_processed, \"toarray\"):\n",
    "    y_processed = y_processed.toarray()\n",
    "\n",
    "# Reshape X and y for LSTM\n",
    "X_processed = X_processed.reshape((X_processed.shape[0], 1, X_processed.shape[1]))\n",
    "y_processed = y_processed.reshape((y_processed.shape[0], 1, y_processed.shape[1]))\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y_processed, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "def create_dataloader(X, y, batch_size=8):\n",
    "    dataset = TensorDataset(X, y)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_loader = create_dataloader(X_train_tensor, y_train_tensor)\n",
    "test_loader = create_dataloader(X_test_tensor, y_test_tensor, batch_size=16)\n",
    "\n",
    "# Define LSTM Model\n",
    "class FanUnitLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_categorical, output_numerical):\n",
    "        super(FanUnitLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.dense = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.categorical_output = nn.Linear(hidden_dim, output_categorical)\n",
    "        self.numerical_output = nn.Linear(hidden_dim, output_numerical)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(x[:, -1, :])\n",
    "        x = torch.relu(self.dense(x))\n",
    "        categorical_out = torch.softmax(self.categorical_output(x), dim=1)\n",
    "        numerical_out = self.numerical_output(x)\n",
    "        return categorical_out, numerical_out\n",
    "\n",
    "# Model parameters\n",
    "input_dim = X_train_tensor.shape[2]\n",
    "hidden_dim = 64\n",
    "output_categorical = len(cat_cols)\n",
    "output_numerical = len(num_cols)\n",
    "\n",
    "# Instantiate the model\n",
    "model = FanUnitLSTM(input_dim, hidden_dim, output_categorical, output_numerical).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "categorical_loss_fn = nn.CrossEntropyLoss()\n",
    "numerical_loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, optimizer, epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_categorical = y_batch[:, :, :output_categorical].squeeze(1)\n",
    "            y_numerical = y_batch[:, :, output_categorical:].squeeze(1)\n",
    "\n",
    "            pred_categorical, pred_numerical = model(X_batch)\n",
    "\n",
    "            loss_categorical = categorical_loss_fn(pred_categorical, y_categorical.argmax(dim=1))\n",
    "            loss_numerical = numerical_loss_fn(pred_numerical, y_numerical)\n",
    "            \n",
    "            loss = loss_categorical + loss_numerical\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, optimizer, epochs=50)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'models/FanUnitLSTM.pth')\n",
    "\n",
    "# Load and evaluate\n",
    "model.load_state_dict(torch.load('models/FanUnitLSTM.pth'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab18315-56bb-4015-b233-c2bcdb60493f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
